<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,IE=9,chrome=1"><meta name="generator" content="MATLAB R2020a"><title>Neural networks</title><style type="text/css">.rtcContent { padding: 30px; } .S0 { margin: 3px 10px 5px 4px; padding: 0px; line-height: 28.8px; min-height: 0px; white-space: pre-wrap; color: rgb(213, 80, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 24px; font-weight: normal; text-align: left;  }
.S1 { margin: 2px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: normal; text-align: left;  }
.CodeBlock { background-color: #F7F7F7; margin: 10px 0 10px 0;}
.S2 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 1px solid rgb(233, 233, 233); border-bottom: 0px none rgb(0, 0, 0); border-radius: 4px 4px 0px 0px; padding: 6px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S3 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 0px none rgb(0, 0, 0); border-radius: 0px; padding: 0px 45px 0px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S4 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 0px 0px 4px 4px; padding: 0px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S5 { margin: 10px 10px 9px 4px; padding: 0px; line-height: 21px; min-height: 0px; white-space: pre-wrap; color: rgb(0, 0, 0); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 14px; font-weight: normal; text-align: left;  }
.S6 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 1px solid rgb(233, 233, 233); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 4px 4px 0px 0px; padding: 6px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S7 { color: rgb(64, 64, 64); padding: 10px 0px 6px 17px; background: rgb(255, 255, 255) none repeat scroll 0% 0% / auto padding-box border-box; font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px; overflow-x: hidden; line-height: 17.234px;  }
.embeddedOutputsErrorElement {min-height: 18px; max-height: 250px; overflow: auto;}
.embeddedOutputsErrorElement.inlineElement {}
.embeddedOutputsErrorElement.rightPaneElement {}
.embeddedOutputsWarningElement{min-height: 18px; max-height: 250px; overflow: auto;}
.embeddedOutputsWarningElement.inlineElement {}
.embeddedOutputsWarningElement.rightPaneElement {}
.diagnosticMessage-wrapper {font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 12px;}
.diagnosticMessage-wrapper.diagnosticMessage-warningType {color: rgb(255,100,0);}
.diagnosticMessage-wrapper.diagnosticMessage-warningType a {color: rgb(255,100,0); text-decoration: underline;}
.diagnosticMessage-wrapper.diagnosticMessage-errorType {color: rgb(230,0,0);}
.diagnosticMessage-wrapper.diagnosticMessage-errorType a {color: rgb(230,0,0); text-decoration: underline;}
.diagnosticMessage-wrapper .diagnosticMessage-messagePart,.diagnosticMessage-wrapper .diagnosticMessage-causePart {white-space: pre-wrap;}
.diagnosticMessage-wrapper .diagnosticMessage-stackPart {white-space: pre;}
.embeddedOutputsTextElement,.embeddedOutputsVariableStringElement {white-space: pre; word-wrap: initial; min-height: 18px; max-height: 250px; overflow: auto;}
.textElement,.rtcDataTipElement .textElement {padding-top: 3px;}
.embeddedOutputsTextElement.inlineElement,.embeddedOutputsVariableStringElement.inlineElement {}
.inlineElement .textElement {}
.embeddedOutputsTextElement.rightPaneElement,.embeddedOutputsVariableStringElement.rightPaneElement {min-height: 16px;}
.rightPaneElement .textElement {padding-top: 2px; padding-left: 9px;}
.S8 { border-left: 1px solid rgb(233, 233, 233); border-right: 1px solid rgb(233, 233, 233); border-top: 0px none rgb(0, 0, 0); border-bottom: 1px solid rgb(233, 233, 233); border-radius: 0px; padding: 0px 45px 4px 13px; line-height: 17.234px; min-height: 18px; white-space: nowrap; color: rgb(0, 0, 0); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 14px;  }
.S9 { margin: 15px 10px 5px 4px; padding: 0px; line-height: 18px; min-height: 0px; white-space: pre-wrap; color: rgb(60, 60, 60); font-family: Helvetica, Arial, sans-serif; font-style: normal; font-size: 17px; font-weight: bold; text-align: left;  }</style></head><body><div class = rtcContent><h1  class = 'S0'><span>Neural networks</span></h1><div  class = 'S1'><span>This live script implements a feed forward neural network from scratch. The network is used for binary classification. Any number of layers can be used, and any number of hidden units can be used per layer. The tanh activation function is used. The residual sum of squares error function is minimized using gradient descent to find the neural network weights.</span></div><div  class = 'S1'><span>We'll test the network on a very simple problem. You can take this script further by testing the network on more complex data. Here, we generate 2-dimensional data that can be separated by a linear discriminant.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S2'><span style="white-space: pre;"><span>rng(0);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>n = 15;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>x = rand(2,n);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% generate +1/-1 labels separated by a discriminant function</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>w = [.3 .5 -1]';</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>y = zeros(1,n);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>i = 1:n</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  y(i) = sign(w'*[1; x(1,i); x(2,i)]);</span></span></div></div><div class="inlineWrapper"><div  class = 'S4'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div></div><div  class = 'S5'><span>We specify the neural network architecture by defining the weight matrices that initially contain random weights. We'll use 2 layers, but you can use any number of layers you like.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S2'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% h(x) = actfun(b2 + W2' * actfun(b1 + W1' x));</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>L = 2; </span><span style="color: rgb(60, 118, 61);">% number of layers</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>W = cell(L,1);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>W{1} = rand(2,2)-.5;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>W{2} = rand(2,1)-.5;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>b = cell(L,1);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>b{1} = rand(2,1)-.5;</span></span></div></div><div class="inlineWrapper"><div  class = 'S4'><span style="white-space: pre;"><span>b{2} = rand(1,1)-.5;</span></span></div></div></div><div  class = 'S5'><span>Now train the network by calling our own gradient descent function (implemented below).</span></div><div class="CodeBlock"><div class="inlineWrapper outputs"><div  class = 'S6'><span style="white-space: pre;"><span>[W, b] = gradient_descent(x, y, W, b);</span></span></div><div  class = 'S7'><div class="inlineElement eoOutputWrapper embeddedOutputsTextElement scrollableOutput" uid="817ADAFB" data-scroll-top="null" data-scroll-left="null" contenteditable="false" data-width="758" data-height="1467" data-hashorizontaloverflow="false" data-testid="output_0" style="max-height: 261px; width: 788px;"><div class="textElement">     0   18.575115
   100   0.009228
   200   0.004205
   300   0.002712
   400   0.001995
   500   0.001574
   600   0.001298
   700   0.001103
   800   0.000958
   900   0.000846
  1000   0.000758
  1100   0.000685
  1200   0.000625
  1300   0.000575
  1400   0.000532
  1500   0.000495
  1600   0.000462
  1700   0.000434
  1800   0.000409
  1900   0.000386
  2000   0.000366
  2100   0.000348
  2200   0.000331
  2300   0.000316
  2400   0.000302
  2500   0.000290
  2600   0.000278
  2700   0.000267
  2800   0.000257
  2900   0.000248
  3000   0.000239
  3100   0.000231
  3200   0.000224
  3300   0.000216
  3400   0.000210
  3500   0.000204
  3600   0.000198
  3700   0.000192
  3800   0.000187
  3900   0.000182
  4000   0.000177
  4100   0.000172
  4200   0.000168
  4300   0.000164
  4400   0.000160
  4500   0.000156
  4600   0.000153
  4700   0.000150
  4800   0.000146
  4900   0.000143
  5000   0.000140
  5100   0.000137
  5200   0.000135
  5300   0.000132
  5400   0.000129
  5500   0.000127
  5600   0.000125
  5700   0.000122
  5800   0.000120
  5900   0.000118
  6000   0.000116
  6100   0.000114
  6200   0.000112
  6300   0.000110
  6400   0.000108
  6500   0.000107
  6600   0.000105
  6700   0.000103
  6800   0.000102
  6900   0.000100
  7000   0.000099
  7100   0.000097
  7200   0.000096
  7300   0.000094
  7400   0.000093
  7500   0.000092
  7600   0.000091
  7700   0.000089
  7800   0.000088
  7900   0.000087
  8000   0.000086
  8100   0.000085
  8200   0.000084
  8300   0.000083
  8400   0.000082
  8500   0.000081
  8600   0.000080
  8700   0.000079
  8800   0.000078
  8900   0.000077
  9000   0.000076
  9100   0.000075
  9200   0.000074
  9300   0.000073
  9400   0.000073
  9500   0.000072
  9600   0.000071
  9700   0.000070
  9800   0.000069
  9900   0.000069
 10000   0.000068</div></div></div></div></div><div  class = 'S5'><span>Plot the training points.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S2'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>i = 1:size(x,2)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">if </span><span>y(i) &gt; 0</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    plot(x(1,i), x(2,i), </span><span style="color: rgb(160, 32, 240);">'bo'</span><span>); hold </span><span style="color: rgb(160, 32, 240);">on</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">else</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    plot(x(1,i), x(2,i), </span><span style="color: rgb(160, 32, 240);">'kx'</span><span>); hold </span><span style="color: rgb(160, 32, 240);">on</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S4'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div></div><div  class = 'S5'><span>Predict and plot labels for random test points.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S2'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>i = 1:100</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  x = rand(2,1);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  h = neuralnet_predict(x, W, b);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">if </span><span>h &gt; 0</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    plot(x(1), x(2), </span><span style="color: rgb(160, 32, 240);">'co'</span><span>); hold </span><span style="color: rgb(160, 32, 240);">on</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">else</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    plot(x(1), x(2), </span><span style="color: rgb(160, 32, 240);">'mx'</span><span>); hold </span><span style="color: rgb(160, 32, 240);">on</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper outputs"><div  class = 'S8'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div><div  class = 'S7'><div class="inlineElement eoOutputWrapper embeddedOutputsFigure" uid="C6AE0125" data-scroll-top="null" data-scroll-left="null" contenteditable="false" data-testid="output_1" style="width: 788px;"><div class="figureElement"><img class="figureImage figureContainingNode" src="data:image/png;base64,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" style="width: 560px;"></div></div></div></div></div><div  class = 'S5'><span>For this training data, the neural network seems to classify points using a linear discriminant. You can check if this is really true by classifying a very large number of points. How does this discriminant compare to the ground truth discriminant used to generate the data?</span></div><div  class = 'S1'><span>Here is the code for gradient descent.</span></div><div class="CodeBlock"><div class="inlineWrapper"><div  class = 'S2'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>[W, b] = gradient_descent(x, y, W, b)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% print initial objective value</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>v = neuralnet_lossfun(x, y, W, b);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>fprintf(</span><span style="color: rgb(160, 32, 240);">'%6d   %f\n'</span><span>, 0, v);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>eta = 0.1;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>i = 1:10000</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  [dW, db] = neuralnet_gradient(x, y, W, b);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">for </span><span>l = 1:length(W)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    </span><span style="color: rgb(60, 118, 61);">% W = W - eta*dW;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    </span><span style="color: rgb(60, 118, 61);">% b = b - eta*db;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    W{l} = W{l} - eta*dW{l};</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    b{l} = b{l} - eta*db{l};</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  v = neuralnet_lossfun(x, y, W, b);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">if </span><span>mod(i,100) == 0</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    fprintf(</span><span style="color: rgb(160, 32, 240);">'%6d   %f\n'</span><span>, i, v);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>v = neuralnet_lossfun(x, y, W, b)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>v = 0;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>p = 1:size(x,2)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  h = neuralnet_predict(x(:,p), W, b);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  v = v + (h - y(p))^2;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>x = neuralnet_predict(x, W, b)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>l = 1:length(W)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  x = actfun(b{l} + W{l}'*x);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>[dW, db] = neuralnet_gradient(x, y, W, b)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% number of layers</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>L = length(W);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% parts of the gradient</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>dW = cell(L,1);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>db = cell(L,1);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>l = 1:L</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  dW{l} = zeros(size(W{l}));</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  db{l} = zeros(size(b{l}));</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>s = cell(L,1); </span><span style="color: rgb(60, 118, 61);">% activations</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>z = cell(L,1); </span><span style="color: rgb(60, 118, 61);">% post-activations</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>d = cell(L,1); </span><span style="color: rgb(60, 118, 61);">% sensitivities</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% loop over data points</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% an optimization is to make this the inner loop</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>n = size(x,2);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">for </span><span>p = 1:n</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(60, 118, 61);">% forward propagation</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  temp = x(:,p);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">for </span><span>l = 1:L</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    s{l} = b{l} + W{l}'*temp;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    z{l} = actfun(s{l});</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    temp = z{l};</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(60, 118, 61);">% back propagation to compute sensitivities;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(60, 118, 61);">% note biases are not used</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  d{L} = dactfun(s{L});</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">for </span><span>l = L-1:-1:1</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    d{l} = (W{l+1}*d{l+1}) .* dactfun(s{l});</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(60, 118, 61);">% compute gradient (accumulate for each data point)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  errorfun_factor = 2*(z{L} - y(p)); </span><span style="color: rgb(60, 118, 61);">% could premultiply into each d{l} if many layers</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  temp = x(:,p);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">for </span><span>l = 1:L</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    dW{l} = dW{l} + temp*d{l}'*errorfun_factor;  </span><span style="color: rgb(60, 118, 61);">% outer product</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    db{l} = db{l} + d{l}*errorfun_factor;</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>    temp = z{l};</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>  </span><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>z = actfun(s)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% activation function</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>z = tanh(s);</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">function </span><span>z = dactfun(s)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span style="color: rgb(60, 118, 61);">% derivative of activation function</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>z = tanh(s); </span><span style="color: rgb(60, 118, 61);">% an optimization is to not compute tanh again (computed already in actfun)</span></span></div></div><div class="inlineWrapper"><div  class = 'S3'><span style="white-space: pre;"><span>z = 1 - z.*z;</span></span></div></div><div class="inlineWrapper"><div  class = 'S4'><span style="white-space: pre;"><span style="color: rgb(0, 0, 255);">end</span></span></div></div></div><h3  class = 'S9'><span>Taking it further</span></h3><div  class = 'S1'><span>How does the code work for more complicated data, in particular data that cannot be separated by a linear discriminant?</span></div><div  class = 'S1'><span>For your more complicated data, also experiment with different numbers of layers and different numbers of units in each layer. Can your network work for your more complicated data?</span></div><div  class = 'S1'><span>Modify the neural network to use the logistic sigmoid activation function in the output layer and also the cross-entropy loss function for the minimization objective.</span></div><div  class = 'S1'><span>See if you an observe a regularization effect by changing the number of hidden units in each layer.</span></div><div  class = 'S1'><span>Modify the neural network to use weight decay for regularization.</span></div><div  class = 'S1'><span></span></div><div  class = 'S1'></div></div>
<br>
<!-- 
##### SOURCE BEGIN #####
%% Neural networks
% This live script implements a feed forward neural network from scratch. The 
% network is used for binary classification. Any number of layers can be used, 
% and any number of hidden units can be used per layer. The tanh activation function 
% is used. The residual sum of squares error function is minimized using gradient 
% descent to find the neural network weights.
% 
% We'll test the network on a very simple problem. You can take this script 
% further by testing the network on more complex data. Here, we generate 2-dimensional 
% data that can be separated by a linear discriminant.

rng(0);
n = 15;
x = rand(2,n);

% generate +1/-1 labels separated by a discriminant function
w = [.3 .5 -1]';
y = zeros(1,n);
for i = 1:n
  y(i) = sign(w'*[1; x(1,i); x(2,i)]);
end
%% 
% We specify the neural network architecture by defining the weight matrices 
% that initially contain random weights. We'll use 2 layers, but you can use any 
% number of layers you like.

% h(x) = actfun(b2 + W2' * actfun(b1 + W1' x));
L = 2; % number of layers
W = cell(L,1);
W{1} = rand(2,2)-.5;
W{2} = rand(2,1)-.5;
b = cell(L,1);
b{1} = rand(2,1)-.5;
b{2} = rand(1,1)-.5;
%% 
% Now train the network by calling our own gradient descent function (implemented 
% below).

[W, b] = gradient_descent(x, y, W, b);
%% 
% Plot the training points.

for i = 1:size(x,2)
  if y(i) > 0
    plot(x(1,i), x(2,i), 'bo'); hold on
  else
    plot(x(1,i), x(2,i), 'kx'); hold on
  end
end
%% 
% Predict and plot labels for random test points.

for i = 1:100
  x = rand(2,1);
  h = neuralnet_predict(x, W, b);
  if h > 0
    plot(x(1), x(2), 'co'); hold on
  else
    plot(x(1), x(2), 'mx'); hold on
  end
end
%% 
% For this training data, the neural network seems to classify points using 
% a linear discriminant. You can check if this is really true by classifying a 
% very large number of points. How does this discriminant compare to the ground 
% truth discriminant used to generate the data?
%% 
% Here is the code for gradient descent.

function [W, b] = gradient_descent(x, y, W, b)

% print initial objective value
v = neuralnet_lossfun(x, y, W, b);
fprintf('%6d   %f\n', 0, v);

eta = 0.1;
for i = 1:10000
  [dW, db] = neuralnet_gradient(x, y, W, b);
  for l = 1:length(W)
    % W = W - eta*dW;
    % b = b - eta*db;
    W{l} = W{l} - eta*dW{l};
    b{l} = b{l} - eta*db{l};
  end
  v = neuralnet_lossfun(x, y, W, b);
  if mod(i,100) == 0
    fprintf('%6d   %f\n', i, v);
  end
end
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function v = neuralnet_lossfun(x, y, W, b)
v = 0;
for p = 1:size(x,2)
  h = neuralnet_predict(x(:,p), W, b);
  v = v + (h - y(p))^2;
end
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function x = neuralnet_predict(x, W, b)
for l = 1:length(W)
  x = actfun(b{l} + W{l}'*x);
end
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function [dW, db] = neuralnet_gradient(x, y, W, b)

% number of layers
L = length(W);

% parts of the gradient
dW = cell(L,1);
db = cell(L,1);
for l = 1:L
  dW{l} = zeros(size(W{l}));
  db{l} = zeros(size(b{l}));
end

s = cell(L,1); % activations
z = cell(L,1); % post-activations
d = cell(L,1); % sensitivities

% loop over data points
% an optimization is to make this the inner loop
n = size(x,2);
for p = 1:n

  % forward propagation
  temp = x(:,p);
  for l = 1:L
    s{l} = b{l} + W{l}'*temp;
    z{l} = actfun(s{l});
    temp = z{l};
  end

  % back propagation to compute sensitivities;
  % note biases are not used
  d{L} = dactfun(s{L});
  for l = L-1:-1:1
    d{l} = (W{l+1}*d{l+1}) .* dactfun(s{l});
  end

  % compute gradient (accumulate for each data point)
  errorfun_factor = 2*(z{L} - y(p)); % could premultiply into each d{l} if many layers
  temp = x(:,p);
  for l = 1:L
    dW{l} = dW{l} + temp*d{l}'*errorfun_factor;  % outer product
    db{l} = db{l} + d{l}*errorfun_factor;
    temp = z{l};
  end

end
end

function z = actfun(s)
% activation function
z = tanh(s);
end

function z = dactfun(s)
% derivative of activation function
z = tanh(s); % an optimization is to not compute tanh again (computed already in actfun)
z = 1 - z.*z;
end
% Taking it further
% How does the code work for more complicated data, in particular data that 
% cannot be separated by a linear discriminant?
% 
% For your more complicated data, also experiment with different numbers of 
% layers and different numbers of units in each layer. Can your network work for 
% your more complicated data?
% 
% Modify the neural network to use the logistic sigmoid activation function 
% in the output layer and also the cross-entropy loss function for the minimization 
% objective.
% 
% See if you an observe a regularization effect by changing the number of hidden 
% units in each layer.
% 
% Modify the neural network to use weight decay for regularization.
% 
% 
% 
%
##### SOURCE END #####
--></body></html>